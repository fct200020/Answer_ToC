{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "题目一：线性回归模型实现\n",
    "问题：实现一个简单的单变量线性回归模型，用于预测城市人口与食品卡车利润之间的关系。数据集包含城市人口（x）和对应的利润（y）。请实现模型训练、可视化和预测功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 加载数据集，假设数据存储在ex1data1.txt文件中\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 第一列是城市人口，第二列是利润\u001b[39;00m\n\u001b[1;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mex1data1.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# 读取无标题的CSV数据文件\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/__init__.py:138\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    121\u001b[0m     concat,\n\u001b[1;32m    122\u001b[0m     lreshape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     qcut,\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_print_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# excel\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     ExcelFile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     read_spss,\n\u001b[1;32m    172\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/testing.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mPublic testing utility functions.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     assert_extension_array_equal,\n\u001b[1;32m      8\u001b[0m     assert_frame_equal,\n\u001b[1;32m      9\u001b[0m     assert_index_equal,\n\u001b[1;32m     10\u001b[0m     assert_series_equal,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_extension_array_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_frame_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_series_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_index_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_testing/__init__.py:914\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytest\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pytest\u001b[38;5;241m.\u001b[39mraises(expected_exception, match\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 914\u001b[0m cython_table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mcommon\u001b[38;5;241m.\u001b[39m_cython_table\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cython_table_params\u001b[39m(ndframe, func_names_and_expected):\n\u001b[1;32m    918\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    Combine frame, functions from com._cython_table\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m    keys and expected result.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m        List of three items (DataFrame, function, expected result)\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# 加载数据集，假设数据存储在ex1data1.txt文件中\n",
    "# 第一列是城市人口，第二列是利润\n",
    "data = pd.read_csv('ex1data1.txt', header=None)  # 读取无标题的CSV数据文件\n",
    "X = data.iloc[:, 0].values.reshape(-1, 1)  # 提取第一列作为特征X，并重塑为(n_samples, 1)的形状\n",
    "y = data.iloc[:, 1].values  # 提取第二列作为目标变量y\n",
    "\n",
    "# 数据可视化\n",
    "plt.figure(figsize=(10, 6))  # 创建一个10x6英寸的图形\n",
    "plt.scatter(X, y, color='red', marker='x', label='训练数据')  # 绘制散点图\n",
    "plt.xlabel('城市人口（万人）')  # 设置x轴标签\n",
    "plt.ylabel('利润（万元）')  # 设置y轴标签\n",
    "plt.title('城市人口与食品卡车利润关系')  # 设置图表标题\n",
    "plt.legend()  # 显示图例\n",
    "plt.grid(True)  # 显示网格线\n",
    "\n",
    "# 创建并训练线性回归模型\n",
    "model = LinearRegression()  # 初始化线性回归模型\n",
    "model.fit(X, y)  # 使用训练数据拟合模型\n",
    "\n",
    "# 获取模型参数\n",
    "w = model.coef_[0]  # 获取权重系数\n",
    "b = model.intercept_  # 获取截距\n",
    "print(f\"模型参数: w = {w:.4f}, b = {b:.4f}\")  # 打印模型参数\n",
    "\n",
    "# 在图上绘制拟合线\n",
    "X_plot = np.array([min(X), max(X)])  # 创建用于绘图的X值范围\n",
    "y_plot = model.predict(X_plot.reshape(-1, 1))  # 计算对应的预测y值\n",
    "plt.plot(X_plot, y_plot, color='blue', label='拟合线')  # 绘制拟合线\n",
    "plt.legend()  # 更新图例\n",
    "plt.show()  # 显示图表\n",
    "\n",
    "# 使用模型进行预测\n",
    "population = 7.0  # 设定要预测的城市人口（万人）\n",
    "predicted_profit = model.predict([[population]])[0]  # 使用模型预测利润\n",
    "print(f\"人口为{population}万的城市预计利润为: {predicted_profit:.2f}万元\")  # 打印预测结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "题目二：决策树模型实现与信息熵计算\n",
    "问题：实现一个决策树分类器，用于鸢尾花分类问题。首先实现信息熵的计算函数，然后使用决策树模型训练并评估准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from math import log\n",
    "\n",
    "# 函数：计算数据集的信息熵\n",
    "def calc_info_entropy(data_labels):\n",
    "    \"\"\"\n",
    "    计算数据集的信息熵\n",
    "    参数:\n",
    "        data_labels: 数据集的标签\n",
    "    返回:\n",
    "        info_entropy: 信息熵值\n",
    "    \"\"\"\n",
    "    num_entries = len(data_labels)  # 获取数据集样本数量\n",
    "    label_counts = {}  # 创建字典，用于存储每个类别的样本数量\n",
    "    \n",
    "    # 统计每个类别的样本数量\n",
    "    for label in data_labels:\n",
    "        if label not in label_counts:  # 如果该类别还未在字典中\n",
    "            label_counts[label] = 0  # 初始化为0\n",
    "        label_counts[label] += 1  # 该类别的计数加1\n",
    "    \n",
    "    # 计算信息熵\n",
    "    info_entropy = 0.0  # 初始化信息熵为0\n",
    "    for label in label_counts:\n",
    "        prob = float(label_counts[label]) / num_entries  # 计算该类别的概率\n",
    "        info_entropy -= prob * log(prob, 2)  # 应用信息熵公式：-p*log2(p)\n",
    "    \n",
    "    return info_entropy  # 返回计算得到的信息熵\n",
    "\n",
    "# 主程序\n",
    "# 1. 加载鸢尾花数据集\n",
    "iris = datasets.load_iris()  # 加载sklearn内置的鸢尾花数据集\n",
    "X = iris.data  # 获取特征数据\n",
    "y = iris.target  # 获取标签数据\n",
    "\n",
    "# 2. 计算原始数据集的信息熵\n",
    "original_entropy = calc_info_entropy(y)  # 使用我们实现的函数计算信息熵\n",
    "print(f\"原始数据集的信息熵: {original_entropy:.4f}\")  # 打印信息熵结果\n",
    "\n",
    "# 3. 分割训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42  # 划分训练集和测试集，测试集占30%\n",
    ")\n",
    "\n",
    "# 4. 创建决策树模型\n",
    "tree_depth = 3  # 设置决策树最大深度为3\n",
    "dtc = DecisionTreeClassifier(max_depth=tree_depth)  # 创建决策树分类器实例\n",
    "\n",
    "# 5. 训练模型\n",
    "dtc.fit(X_train, y_train)  # 使用训练集训练决策树模型\n",
    "\n",
    "# 6. 在测试集上进行预测\n",
    "y_pred = dtc.predict(X_test)  # 使用训练好的模型对测试集进行预测\n",
    "\n",
    "# 7. 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)  # 计算预测的准确率\n",
    "print(f\"决策树(深度={tree_depth})模型的测试准确率: {accuracy:.4f}\")  # 打印准确率\n",
    "\n",
    "# 8. 查看决策树的一些属性\n",
    "print(f\"决策树的特征重要性: {dtc.feature_importances_}\")  # 打印特征重要性\n",
    "print(f\"决策树的叶节点数量: {dtc.get_n_leaves()}\")  # 打印叶节点数量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "题目三：多层感知机实现手写数字识别\n",
    "问题：使用PyTorch框架实现一个多层感知机(MLP)神经网络，用于MNIST手写数字识别任务。网络应包含输入层、隐藏层和输出层，并使用反向传播算法进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 设置随机种子以确保结果可重现\n",
    "torch.manual_seed(42)  # 设置随机数生成器的种子为42\n",
    "\n",
    "# 定义超参数\n",
    "input_size = 784  # 输入特征的维度：28x28=784\n",
    "hidden_size = 128  # 隐藏层神经元数量\n",
    "output_size = 10  # 输出类别数量（0-9数字）\n",
    "batch_size = 100  # 每批处理的样本数\n",
    "learning_rate = 0.001  # 学习率\n",
    "num_epochs = 5  # 训练轮数\n",
    "\n",
    "# 准备MNIST数据集\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为PyTorch张量\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # 使用MNIST的均值和标准差进行标准化\n",
    "])\n",
    "\n",
    "# 下载并加载训练集\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',  # 数据存储路径\n",
    "    train=True,  # 使用训练集\n",
    "    transform=transform,  # 应用上面定义的变换\n",
    "    download=True  # 如果数据不存在，则下载\n",
    ")\n",
    "\n",
    "# 下载并加载测试集\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',  \n",
    "    train=False,  # 使用测试集\n",
    "    transform=transform  \n",
    ")\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,  \n",
    "    batch_size=batch_size,  \n",
    "    shuffle=True  # 打乱数据顺序\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False  # 测试集不需要打乱\n",
    ")\n",
    "\n",
    "# 定义多层感知机模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()  # 调用父类构造函数\n",
    "        \n",
    "        # 定义网络层\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 第一个全连接层\n",
    "        self.relu = nn.ReLU()  # ReLU激活函数\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # 第二个全连接层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播函数\"\"\"\n",
    "        x = x.view(-1, input_size)  # 将输入展平为(batch_size, 784)\n",
    "        x = self.fc1(x)  # 第一层线性变换\n",
    "        x = self.relu(x)  # 应用ReLU激活函数\n",
    "        x = self.fc2(x)  # 第二层线性变换\n",
    "        return x  # 返回输出\n",
    "\n",
    "# 实例化模型、损失函数和优化器\n",
    "model = MLP(input_size, hidden_size, output_size)  # 创建MLP模型实例\n",
    "criterion = nn.CrossEntropyLoss()  # 使用交叉熵损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # 使用Adam优化器\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 前向传播\n",
    "        outputs = model(images)  # 获取模型预测输出\n",
    "        loss = criterion(outputs, labels)  # 计算损失\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()  # 清空之前的梯度\n",
    "        loss.backward()  # 反向传播计算梯度\n",
    "        optimizer.step()  # 更新模型参数\n",
    "        \n",
    "        # 打印训练信息\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 测试模型\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "with torch.no_grad():  # 在评估时不需要计算梯度\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)  # 获取模型输出\n",
    "        _, predicted = torch.max(outputs.data, 1)  # 获取最高概率的类别索引\n",
    "        total += labels.size(0)  # 计算总样本数\n",
    "        correct += (predicted == labels).sum().item()  # 计算正确预测的样本数\n",
    "    \n",
    "    print(f'测试集准确率: {100 * correct / total:.2f}%')  # 打印测试准确率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "题目四：卷积神经网络实现MNIST图像分类\n",
    "问题：实现一个卷积神经网络(CNN)用于MNIST手写数字识别。网络应包含卷积层、池化层和全连接层，并使用dropout防止过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 设置超参数\n",
    "learning_rate = 1e-3  # 学习率\n",
    "keep_prob_rate = 0.5  # dropout保留率\n",
    "num_epochs = 3  # 训练轮数\n",
    "batch_size = 64  # 每批处理的样本数\n",
    "\n",
    "# 数据准备\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为张量\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # 标准化\n",
    "])\n",
    "\n",
    "# 加载训练集\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',  # 数据存储路径\n",
    "    train=True,  # 使用训练集\n",
    "    transform=transform,  # 应用变换\n",
    "    download=True  # 如果不存在则下载\n",
    ")\n",
    "\n",
    "# 加载测试集\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True  # 打乱数据\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 定义CNN模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()  # 调用父类构造函数\n",
    "        \n",
    "        # 第一个卷积块：卷积->激活->池化\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=2),  # 输入1通道，输出32通道，5x5卷积核\n",
    "            nn.ReLU(),  # ReLU激活函数\n",
    "            nn.MaxPool2d(kernel_size=2)  # 2x2最大池化\n",
    "        )\n",
    "        \n",
    "        # 第二个卷积块：卷积->激活->池化\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),  # 输入32通道，输出64通道，3x3卷积核\n",
    "            nn.ReLU(),  # ReLU激活函数\n",
    "            nn.MaxPool2d(kernel_size=2)  # 2x2最大池化\n",
    "        )\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 1024)  # 全连接层，输入维度是64*7*7，输出维度是1024\n",
    "        self.dropout = nn.Dropout(p=1-keep_prob_rate)  # Dropout层，防止过拟合\n",
    "        self.fc2 = nn.Linear(1024, 10)  # 输出层，10个类别\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播函数\"\"\"\n",
    "        x = self.conv1(x)  # 第一个卷积块\n",
    "        x = self.conv2(x)  # 第二个卷积块\n",
    "        x = x.view(x.size(0), -1)  # 展平特征图\n",
    "        x = self.fc1(x)  # 第一个全连接层\n",
    "        x = torch.relu(x)  # ReLU激活\n",
    "        x = self.dropout(x)  # 应用dropout\n",
    "        x = self.fc2(x)  # 第二个全连接层\n",
    "        return x  # 返回输出\n",
    "\n",
    "# 实例化模型、损失函数和优化器\n",
    "model = CNN()  # 创建CNN模型实例\n",
    "criterion = nn.CrossEntropyLoss()  # 使用交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # 使用Adam优化器\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 设置为训练模式\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 前向传播\n",
    "        outputs = model(images)  # 获取模型输出\n",
    "        loss = criterion(outputs, labels)  # 计算损失\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "        \n",
    "        running_loss += loss.item()  # 累加损失\n",
    "        \n",
    "        # 打印训练信息\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "# 测试模型\n",
    "model.eval()  # 设置为评估模式\n",
    "with torch.no_grad():  # 不计算梯度\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)  # 获取输出\n",
    "        _, predicted = torch.max(outputs.data, 1)  # 获取预测类别\n",
    "        total += labels.size(0)  # 计算样本总数\n",
    "        correct += (predicted == labels).sum().item()  # 计算正确预测数\n",
    "    \n",
    "    print(f'测试集准确率: {100 * correct / total:.2f}%')  # 输出测试准确率\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'cnn_model.pth')  # 保存模型参数\n",
    "print(\"模型已保存\")  # 输出保存成功提示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "题目六：LSTM实现时间序列预测\n",
    "问题：使用LSTM（长短期记忆网络）创建一个时间序列预测模型，预测未来的股票价格。请实现数据准备、模型构建、训练和预测功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设置随机种子以确保结果可重现\n",
    "torch.manual_seed(42)  # 设置PyTorch的随机种子\n",
    "np.random.seed(42)  # 设置NumPy的随机种子\n",
    "\n",
    "# 数据准备函数\n",
    "def prepare_data(data, look_back=60):\n",
    "    \"\"\"\n",
    "    准备LSTM模型的训练和测试数据\n",
    "    \"\"\"\n",
    "    # 数据归一化\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))  # 创建归一化器，将数据缩放到0-1范围\n",
    "    scaled_data = scaler.fit_transform(data.reshape(-1, 1))  # 将数据转换为2D并归一化\n",
    "    \n",
    "    # 创建时间序列数据集\n",
    "    x, y = [], []  # 初始化特征和目标列表\n",
    "    for i in range(len(scaled_data) - look_back):\n",
    "        x.append(scaled_data[i:i + look_back, 0])  # 添加look_back天的数据作为特征\n",
    "        y.append(scaled_data[i + look_back, 0])  # 添加第look_back+1天的数据作为目标\n",
    "    \n",
    "    # 转换为numpy数组\n",
    "    x = np.array(x)  # 将特征列表转换为NumPy数组\n",
    "    y = np.array(y)  # 将目标列表转换为NumPy数组\n",
    "    \n",
    "    # 调整数据形状为LSTM输入格式: [samples, time steps, features]\n",
    "    x = np.reshape(x, (x.shape[0], x.shape[1], 1))  # 重塑为LSTM所需的3D输入\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    train_size = int(len(x) * 0.8)  # 80%的数据用于训练\n",
    "    x_train, x_test = x[:train_size], x[train_size:]  # 划分特征训练集和测试集\n",
    "    y_train, y_test = y[:train_size], y[train_size:]  # 划分目标训练集和测试集\n",
    "    \n",
    "    # 转换为PyTorch张量\n",
    "    x_train = torch.FloatTensor(x_train)  # 转换为PyTorch浮点张量\n",
    "    y_train = torch.FloatTensor(y_train)  # 转换为PyTorch浮点张量\n",
    "    x_test = torch.FloatTensor(x_test)  # 转换为PyTorch浮点张量\n",
    "    y_test = torch.FloatTensor(y_test)  # 转换为PyTorch浮点张量\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, scaler  # 返回处理后的数据和归一化器\n",
    "\n",
    "# 定义LSTM模型\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super(LSTM, self).__init__()  # 调用父类构造函数\n",
    "        self.hidden_layer_size = hidden_layer_size  # 设置隐藏层大小\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)  # LSTM层，输入大小和隐藏层大小\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "题目六：LSTM实现时间序列预测\n",
    "问题：使用LSTM（长短期记忆网络）创建一个时间序列预测模型，预测未来的股票价格。请实现数据准备、模型构建、训练和预测功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设置随机种子以确保结果可重现\n",
    "torch.manual_seed(42)  # 设置PyTorch的随机种子\n",
    "np.random.seed(42)  # 设置NumPy的随机种子\n",
    "\n",
    "# 数据准备函数\n",
    "def prepare_data(data, look_back=60):\n",
    "    \"\"\"\n",
    "    准备LSTM模型的训练和测试数据\n",
    "    \"\"\"\n",
    "    # 数据归一化\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))  # 创建归一化器，将数据缩放到0-1范围\n",
    "    scaled_data = scaler.fit_transform(data.reshape(-1, 1))  # 将数据转换为2D并归一化\n",
    "    \n",
    "    # 创建时间序列数据集\n",
    "    x, y = [], []  # 初始化特征和目标列表\n",
    "    for i in range(len(scaled_data) - look_back):\n",
    "        x.append(scaled_data[i:i + look_back, 0])  # 添加look_back天的数据作为特征\n",
    "        y.append(scaled_data[i + look_back, 0])  # 添加第look_back+1天的数据作为目标\n",
    "    \n",
    "    # 转换为numpy数组\n",
    "    x = np.array(x)  # 将特征列表转换为NumPy数组\n",
    "    y = np.array(y)  # 将目标列表转换为NumPy数组\n",
    "    \n",
    "    # 调整数据形状为LSTM输入格式: [samples, time steps, features]\n",
    "    x = np.reshape(x, (x.shape[0], x.shape[1], 1))  # 重塑为LSTM所需的3D输入\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    train_size = int(len(x) * 0.8)  # 80%的数据用于训练\n",
    "    x_train, x_test = x[:train_size], x[train_size:]  # 划分特征训练集和测试集\n",
    "    y_train, y_test = y[:train_size], y[train_size:]  # 划分目标训练集和测试集\n",
    "    \n",
    "    # 转换为PyTorch张量\n",
    "    x_train = torch.FloatTensor(x_train)  # 转换为PyTorch浮点张量\n",
    "    y_train = torch.FloatTensor(y_train)  # 转换为PyTorch浮点张量\n",
    "    x_test = torch.FloatTensor(x_test)  # 转换为PyTorch浮点张量\n",
    "    y_test = torch.FloatTensor(y_test)  # 转换为PyTorch浮点张量\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, scaler  # 返回处理后的数据和归一化器\n",
    "\n",
    "# 定义LSTM模型\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super(LSTM, self).__init__()  # 调用父类构造函数\n",
    "        self.hidden_layer_size = hidden_layer_size  # 设置隐藏层大小\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)  # LSTM层，输入大小和隐藏层大小\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)  # 线性层，隐藏层大小到输出大小\n",
    "        \n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        self.hidden_cell = (torch.zeros(1, 1, self.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, self.hidden_layer_size))  # 初始化为零张量\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"前向传播函数\"\"\"\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)  # LSTM前向传播\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))  # 通过线性层\n",
    "        return predictions[-1]  # 返回最后一个时间步的预测\n",
    "\n",
    "# 训练模型函数\n",
    "def train_model(model, x_train, y_train, epochs=100, lr=0.001):\n",
    "    \"\"\"训练LSTM模型\"\"\"\n",
    "    loss_function = nn.MSELoss()  # 使用均方误差损失函数\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # 使用Adam优化器\n",
    "    \n",
    "    epochs_losses = []  # 存储每个epoch的损失\n",
    "    for i in range(epochs):\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))  # 在每个epoch开始前重置隐藏状态\n",
    "        \n",
    "        y_pred = model(x_train)  # 获取模型预测\n",
    "        \n",
    "        loss = loss_function(y_pred, y_train)  # 计算损失\n",
    "        \n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "        \n",
    "        epochs_losses.append(loss.item())  # 添加当前epoch的损失\n",
    "        \n",
    "        if i % 10 == 0:  # 每10个epoch打印一次损失\n",
    "            print(f'Epoch: {i}, Loss: {loss.item():.6f}')\n",
    "    \n",
    "    return epochs_losses  # 返回训练过程中的损失\n",
    "\n",
    "# 预测函数\n",
    "def predict(model, x_test, scaler):\n",
    "    \"\"\"使用训练好的模型进行预测\"\"\"\n",
    "    model.eval()  # 设置为评估模式\n",
    "    predictions = []  # 存储预测结果\n",
    "    \n",
    "    with torch.no_grad():  # 不计算梯度\n",
    "        for i in range(len(x_test)):\n",
    "            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                                torch.zeros(1, 1, model.hidden_layer_size))  # 重置隐藏状态\n",
    "            pred = model(x_test[i].unsqueeze(0))  # 获取预测值，增加批次维度\n",
    "            predictions.append(pred.item())  # 添加到预测列表\n",
    "    \n",
    "    # 反归一化\n",
    "    predictions = np.array(predictions).reshape(-1, 1)  # 将预测列表转换为NumPy数组\n",
    "    predictions = scaler.inverse_transform(predictions)  # 反归一化\n",
    "    \n",
    "    return predictions  # 返回预测结果\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 生成模拟股票数据\n",
    "    days = 1000  # 天数\n",
    "    np.random.seed(42)  # 确保可重现性\n",
    "    price = 100 + np.cumsum(np.random.normal(0, 1, days))  # 模拟股票价格\n",
    "    \n",
    "    # 数据准备\n",
    "    look_back = 60  # 使用过去60天的数据预测下一天\n",
    "    x_train, y_train, x_test, y_test, scaler = prepare_data(price, look_back)  # 准备数据\n",
    "    \n",
    "    # 创建并训练模型\n",
    "    model = LSTM()  # 创建LSTM模型\n",
    "    losses = train_model(model, x_train, y_train, epochs=100)  # 训练模型\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))  # 创建图形\n",
    "    plt.plot(losses)  # 绘制损失曲线\n",
    "    plt.title('LSTM Training Loss')  # 设置标题\n",
    "    plt.xlabel('Epochs')  # 设置x轴标签\n",
    "    plt.ylabel('Loss')  # 设置y轴标签\n",
    "    plt.show()  # 显示图形\n",
    "    \n",
    "    # 预测并可视化结果\n",
    "    predictions = predict(model, x_test, scaler)  # 获取预测结果\n",
    "    \n",
    "    # 获取实际值\n",
    "    y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))  # 反归一化测试目标\n",
    "    \n",
    "    # 绘制预测结果\n",
    "    plt.figure(figsize=(12, 6))  # 创建图形\n",
    "    plt.plot(y_test_actual, label='Actual Price')  # 绘制实际价格\n",
    "    plt.plot(predictions, label='Predicted Price')  # 绘制预测价格\n",
    "    plt.title('Stock Price Prediction using LSTM')  # 设置标题\n",
    "    plt.xlabel('Time')  # 设置x轴标签\n",
    "    plt.ylabel('Stock Price')  # 设置y轴标签\n",
    "    plt.legend()  # 显示图例\n",
    "    plt.show()  # 显示图形\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # 执行主函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "题目七：GAN实现手写数字生成\n",
    "问题：实现一个生成对抗网络(GAN)，用于生成MNIST手写数字图像。请实现生成器、判别器、训练过程和结果可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 设置随机种子以确保结果可重现\n",
    "torch.manual_seed(42)  # 设置PyTorch的随机种子\n",
    "np.random.seed(42)  # 设置NumPy的随机种子\n",
    "\n",
    "# 超参数设置\n",
    "latent_dim = 100  # 潜在空间维度\n",
    "img_shape = (1, 28, 28)  # 图像形状：通道x高度x宽度\n",
    "batch_size = 64  # 批量大小\n",
    "lr = 0.0002  # 学习率\n",
    "betas = (0.5, 0.999)  # Adam优化器的beta参数\n",
    "n_epochs = 200  # 训练轮数\n",
    "\n",
    "# 准备MNIST数据集\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为PyTorch张量\n",
    "    transforms.Normalize([0.5], [0.5])  # 标准化到[-1, 1]\n",
    "])\n",
    "\n",
    "# 加载训练集\n",
    "mnist_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',  # 数据存储路径\n",
    "    train=True,  # 使用训练集\n",
    "    transform=transform,  # 应用变换\n",
    "    download=True  # 如果不存在则下载\n",
    ")\n",
    "\n",
    "# 创建数据加载器\n",
    "dataloader = DataLoader(\n",
    "    mnist_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True  # 打乱数据\n",
    ")\n",
    "\n",
    "# 判断是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 确定使用GPU还是CPU\n",
    "print(f\"Using device: {device}\")  # 打印使用的设备\n",
    "\n",
    "# 定义生成器网络\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()  # 调用父类构造函数\n",
    "        \n",
    "        # 定义生成器网络结构\n",
    "        self.model = nn.Sequential(\n",
    "            # 输入是潜在向量，维度为latent_dim\n",
    "            nn.Linear(latent_dim, 128),  # 全连接层，将潜在向量映射到128维\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数\n",
    "            nn.Linear(128, 256),  # 全连接层，128->256\n",
    "            nn.BatchNorm1d(256),  # 批归一化，提高训练稳定性\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数\n",
    "            nn.Linear(256, 512),  # 全连接层，256->512\n",
    "            nn.BatchNorm1d(512),  # 批归一化\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数\n",
    "            nn.Linear(512, 1024),  # 全连接层，512->1024\n",
    "            nn.BatchNorm1d(1024),  # 批归一化\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),  # 全连接层，1024->图像大小(784)\n",
    "            nn.Tanh()  # Tanh激活函数，输出范围为[-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"前向传播函数\"\"\"\n",
    "        img = self.model(z)  # 通过模型生成图像\n",
    "        img = img.view(img.size(0), *img_shape)  # 重塑为(batch_size, 1, 28, 28)\n",
    "        return img  # 返回生成的图像\n",
    "\n",
    "# 定义判别器网络\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()  # 调用父类构造函数\n",
    "        \n",
    "        # 定义判别器网络结构\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),  # 全连接层，图像大小(784)->512\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数\n",
    "            nn.Linear(512, 256),  # 全连接层，512->256\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数\n",
    "            nn.Linear(256, 1),  # 全连接层，256->1\n",
    "            nn.Sigmoid()  # Sigmoid激活函数，输出为真实概率(0-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        \"\"\"前向传播函数\"\"\"\n",
    "        img_flat = img.view(img.size(0), -1)  # 展平图像为(batch_size, 784)\n",
    "        validity = self.model(img_flat)  # 通过模型计算真实概率\n",
    "        return validity  # 返回真实概率\n",
    "\n",
    "# 初始化生成器和判别器\n",
    "generator = Generator().to(device)  # 创建生成器并移至设备\n",
    "discriminator = Discriminator().to(device)  # 创建判别器并移至设备\n",
    "\n",
    "# 损失函数\n",
    "adversarial_loss = nn.BCELoss()  # 二分类交叉熵损失函数\n",
    "\n",
    "# 优化器\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=betas)  # 生成器优化器\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=betas)  # 判别器优化器\n",
    "\n",
    "# 生成固定噪声用于可视化\n",
    "fixed_noise = torch.randn(25, latent_dim, device=device)  # 生成25个随机潜在向量\n",
    "\n",
    "# 保存生成图像的列表\n",
    "generated_images = []  # 用于存储每个epoch生成的图像\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (real_imgs, _) in enumerate(dataloader):\n",
    "        # 配置真实图像和标签\n",
    "        real_imgs = real_imgs.to(device)  # 将真实图像移至设备\n",
    "        valid = torch.ones(real_imgs.size(0), 1, device=device)  # 真实标签为1\n",
    "        fake = torch.zeros(real_imgs.size(0), 1, device=device)  # 生成标签为0\n",
    "        \n",
    "        # -----------------\n",
    "        # 训练生成器\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()  # 清空生成器梯度\n",
    "        \n",
    "        # 生成随机噪声\n",
    "        z = torch.randn(real_imgs.size(0), latent_dim, device=device)  # 生成随机潜在向量\n",
    "        \n",
    "        # 生成假图像\n",
    "        gen_imgs = generator(z)  # 使用生成器生成图像\n",
    "        \n",
    "        # 计算生成器损失：希望判别器将生成的图像视为真实图像\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)  # 计算生成器损失\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        g_loss.backward()  # 反向传播\n",
    "        optimizer_G.step()  # 更新生成器参数\n",
    "        \n",
    "        # -----------------\n",
    "        # 训练判别器\n",
    "        # -----------------\n",
    "        optimizer_D.zero_grad()  # 清空判别器梯度\n",
    "        \n",
    "        # 计算真实图像的判别器损失\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)  # 真实图像应被判为真\n",
    "        \n",
    "        # 计算生成图像的判别器损失\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)  # 生成图像应被判为假\n",
    "        \n",
    "        # 总判别器损失\n",
    "        d_loss = (real_loss + fake_loss) / 2  # 取平均值\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        d_loss.backward()  # 反向传播\n",
    "        optimizer_D.step()  # 更新判别器参数\n",
    "        \n",
    "        # 打印进度\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(dataloader)}] \"\n",
    "                f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\"\n",
    "            )\n",
    "    \n",
    "    # 保存生成的图像\n",
    "    with torch.no_grad():  # 不计算梯度\n",
    "        gen_imgs = generator(fixed_noise).detach().cpu()  # 生成图像并移至CPU\n",
    "        generated_images.append(gen_imgs)  # 添加到列表中\n",
    "    \n",
    "    # 每10个epoch保存和可视化生成的图像\n",
    "    if epoch % 10 == 0:\n",
    "        # 保存模型参数\n",
    "        torch.save(generator.state_dict(), f\"generator_epoch_{epoch}.pth\")\n",
    "        \n",
    "        # 可视化生成的图像\n",
    "        plt.figure(figsize=(5, 5))  # 创建图形\n",
    "        for j in range(25):  # 显示25张图像\n",
    "            plt.subplot(5, 5, j+1)  # 创建子图\n",
    "            plt.imshow(gen_imgs[j, 0, :, :].numpy(), cmap='gray')  # 显示图像\n",
    "            plt.axis('off')  # 关闭坐标轴\n",
    "        plt.savefig(f\"epoch_{epoch}.png\")  # 保存图像\n",
    "        plt.close()  # 关闭图形\n",
    "\n",
    "# 创建GIF动画\n",
    "import imageio\n",
    "\n",
    "# 保存最终训练后的图像\n",
    "plt.figure(figsize=(10, 10))  # 创建图形\n",
    "final_imgs = generated_images[-1]  # 获取最后一个epoch的图像\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(final_imgs[i, 0, :, :].numpy(), cmap='gray')  # 显示图像\n",
    "    plt.axis('off')  # 关闭坐标轴\n",
    "plt.savefig(\"final_generated_digits.png\")  # 保存图像\n",
    "plt.close()  # 关闭图形\n",
    "\n",
    "# 保存训练过程中的图像变化为GIF\n",
    "frames = []  # 用于存储GIF帧\n",
    "for epoch in range(0, n_epochs, 10):  # 每10个epoch选一次\n",
    "    epoch_imgs = generated_images[epoch]  # 获取该epoch的图像\n",
    "    \n",
    "    # 创建图像网格\n",
    "    fig = plt.figure(figsize=(5, 5))  # 创建图形\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(epoch_imgs[i, 0, :, :].numpy(), cmap='gray')  # 显示图像\n",
    "        plt.axis('off')  # 关闭坐标轴\n",
    "    \n",
    "    # 保存图像到内存buffer\n",
    "    from io import BytesIO\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # 读取图像并添加到帧\n",
    "    img = imageio.imread(buf)\n",
    "    frames.append(img)\n",
    "    plt.close()  # 关闭图形\n",
    "\n",
    "# 将帧保存为GIF\n",
    "imageio.mimsave('training_progress.gif', frames, fps=2)  # 保存GIF，fps为每秒帧数\n",
    "\n",
    "print(\"训练完成，生成的图像已保存\")  # 打印完成信息\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
